{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.**\n",
    "\n",
    "**Ans:**\n",
    "\n",
    "Min-max scaling is a data normalization technique that scales features in a dataset to a fixed range of values between 0 and 1. This is done by subtracting the minimum value of the feature from all values and then dividing the result by the range of the feature (i.e., the difference between the maximum and minimum values).\n",
    "\n",
    "The formula for min-max scaling is:\n",
    "\n",
    "x_normalized = (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "where x is the original value, min(x) is the minimum value in the feature, and max(x) is the maximum value in the feature.\n",
    "\n",
    "Min-max scaling is used in data preprocessing to ensure that all features are on the same scale, which is particularly important in machine learning algorithms that are sensitive to the scale of the input data. It is commonly used in image processing, natural language processing, and other domains where feature scaling is important.\n",
    "\n",
    "For example, suppose we have a dataset containing the heights of individuals in centimeters. The minimum height is 150 cm, and the maximum height is 200 cm. We can apply min-max scaling to the dataset by subtracting 150 from each height and dividing the result by 50 (the range of heights):\n",
    "\n",
    "original data: 170, 180, 160, 190, 200, 150\n",
    "\n",
    "min-max scaled data: 0.4, 0.8, 0.0, 1.0, 1.0, 0.0\n",
    "\n",
    "As you can see, the scaled data now falls within the range of 0 to 1, which is a fixed range that is easier to work with in many machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.**\n",
    "\n",
    "**Ans:**\n",
    "\n",
    "The Unit Vector technique in feature scaling, also known as \"normalization,\" involves scaling the features so that each feature has a unit norm (i.e., magnitude of 1). This is done by dividing each feature value by the Euclidean norm of the feature vector.\n",
    "\n",
    "Mathematically, for a feature vector x, the normalized feature vector, x_norm, is given by:\n",
    "x_norm = x / ||x||\n",
    "\n",
    "where ||x|| is the Euclidean norm of x.\n",
    "\n",
    "In contrast, Min-Max scaling involves scaling the features so that they fall within a specified range, usually between 0 and 1. This is done by subtracting the minimum value of the feature from each value and then dividing by the range of the feature.\n",
    "\n",
    "Mathematically, for a feature vector x, the Min-Max scaled feature vector, x_scaled, is given by:\n",
    "x_scaled = (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "The main difference between the Unit Vector technique and Min-Max scaling is the way the features are scaled. While the Unit Vector technique ensures that all features have a magnitude of 1, Min-Max scaling ensures that all features fall within a specified range.\n",
    "\n",
    "An example of the application of the Unit Vector technique is in text classification, where documents are represented as feature vectors using techniques such as Bag-of-Words or TF-IDF. In this case, the Unit Vector technique can be used to normalize the feature vectors so that the length of the document (i.e., the number of words) does not affect the similarity between documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.**\n",
    "\n",
    "**Ans:**\n",
    "\n",
    "PCA, or Principle Component Analysis, is a statistical technique that is commonly used for dimensionality reduction. The goal of PCA is to identify a lower-dimensional representation of the data that captures as much of the variance in the original data as possible.\n",
    "\n",
    "PCA works by identifying the linear combinations of the original features, called principle components, that capture the most variance in the data. These principle components are ordered in terms of their ability to explain the variance in the data, with the first principle component capturing the most variance.\n",
    "\n",
    "PCA can be used in dimensionality reduction by selecting a subset of the principle components that capture the majority of the variance in the data. This allows for a lower-dimensional representation of the data that still captures most of the important information.\n",
    "\n",
    "An example of the application of PCA is in image processing. Images are typically represented as high-dimensional feature vectors, with each pixel being a separate feature. However, many of these features are redundant, as they may be correlated with other features or may not contain much useful information.\n",
    "\n",
    "In this case, PCA can be used to identify a lower-dimensional representation of the image that still captures most of the important information. For example, if the first 50 principle components capture 95% of the variance in the image data, we can use only those 50 components as our new feature representation, rather than the original high-dimensional feature vectors.\n",
    "\n",
    "By doing so, we can reduce the dimensionality of the image data, making it easier to process and analyze, while still capturing most of the important information in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.**\n",
    "\n",
    "**Ans:**\n",
    "\n",
    "PCA (Principal Component Analysis) can be used for feature extraction, which is the process of extracting new features from existing features in a dataset. In feature extraction, we select a subset of the original features that capture the most important information in the data. PCA can help us in this process by identifying the principal components that explain the most variance in the data and using them as new features.\n",
    "\n",
    "For example, suppose we have a dataset with 10 features, and we want to reduce the dimensionality of the dataset to 3 features for building a machine learning model. We can apply PCA for feature extraction as follows:\n",
    "\n",
    "Standardize the data by subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    "Compute the covariance matrix of the standardized data.\n",
    "\n",
    "Calculate the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "Sort the eigenvectors by their corresponding eigenvalues in descending order.\n",
    "\n",
    "Choose the top 3 eigenvectors (principal components) that explain most of the variance in the data.\n",
    "\n",
    "Transform the data into the new 3-dimensional space by multiplying it with the chosen eigenvectors.\n",
    "\n",
    "Use the transformed data as the new features for building a machine learning model.\n",
    "\n",
    "The resulting transformed data contains 3 features, which capture the most important information in the original dataset. These new features can be used for building machine learning models, such as regression or classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.**\n",
    "\n",
    "**Ans:**\n",
    "\n",
    "We clearly know that we have three features price, rating, time and these feature have different unit and it become difficult for us to analyze and visualize beacause the unit are different so we use normalization min max scaling.\n",
    "\n",
    "example:\n",
    "\n",
    "Suppose the dataset contains the following values for the price, rating, and delivery time features:\n",
    "\n",
    "Price: [10, 20, 15, 25, 30] \n",
    "\n",
    "Rating: [2.5, 3.5, 4.0, 3.0, 4.5] \n",
    "\n",
    "Delivery Time: [30, 45, 20, 60, 40]\n",
    "\n",
    "To scale these features using Min-Max scaling, we would perform the following steps:\n",
    "\n",
    "\n",
    "1. Identify the numerical features to be scaled: \n",
    "\n",
    "   Price, Rating, and Delivery Time.\n",
    "   \n",
    "   \n",
    "\n",
    "2. Determine the minimum and maximum values for each of the features:\n",
    "\n",
    "\n",
    "    Price: minimum = 10, maximum = 30\n",
    "\n",
    "\n",
    "    Rating: minimum = 2.5, maximum = 4.5\n",
    "    \n",
    "\n",
    "    Delivery Time: minimum = 20, maximum = 60\n",
    "    \n",
    "    \n",
    "\n",
    "3. Apply the Min-Max scaling formula to scale the features:\n",
    "\n",
    "\n",
    "    Price: [(10-10)/(30-10), (20-10)/(30-10), (15-10)/(30-10), (25-10)/(30-10), (30-10)/(30-10)] = [0.00, 0.33, 0.17, 0.67, 1.00]\n",
    "    \n",
    "\n",
    "    Rating: [(2.5-2.5)/(4.5-2.5), (3.5-2.5)/(4.5-2.5), (4.0-2.5)/(4.5-2.5), (3.0-2.5)/(4.5-2.5), (4.5-2.5)/(4.5-2.5)] = [0.00, 0.50, 0.75, 0.25, 1.00]\n",
    "    \n",
    "\n",
    "    Delivery Time: [(30-20)/(60-20), (45-20)/(60-20), (20-20)/(60-20), (60-20)/(60-20), (40-20)/(60-20)] = [0.25, 0.75, 0.00, 1.00, 0.50]\n",
    "    \n",
    "    \n",
    "\n",
    "4. Replace the original feature values with the scaled values in the dataset. The resulting dataset with the scaled values would be:\n",
    "\n",
    "\n",
    "    Price: [0.00, 0.33, 0.17, 0.67, 1.00]\n",
    "\n",
    "\n",
    "    Rating: [0.00, 0.50, 0.75, 0.25, 1.00]\n",
    "\n",
    "\n",
    "    Delivery Time: [0.25, 0.75, 0.00, 1.00, 0.\n",
    "    \n",
    "    \n",
    "\n",
    "As you can see we scale all the features in range of 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.**\n",
    "\n",
    "**Ans:**\n",
    "\n",
    "PCA (Principal Component Analysis) is a commonly used technique in machine learning to reduce the dimensionality of high-dimensional datasets. It works by identifying the principle components of the data, which are linear combinations of the original features that capture the most variance in the data. These principle components can then be used as the new set of features for further analysis, reducing the dimensionality of the dataset while preserving the most important information.\n",
    "\n",
    "In the case of a stock price prediction model, the dataset may contain many features, such as company financial data, market trends, and other economic indicators. Some of these features may be redundant or correlated with each other, making it difficult to analyze the data or build an accurate prediction model. In this case, we can use PCA to reduce the dimensionality of the dataset and extract the most important information from the data.\n",
    "\n",
    "To use PCA to reduce the dimensionality of the dataset, we first need to standardize the data by subtracting the mean and dividing by the standard deviation. We can then compute the covariance matrix of the data and find the principle components using eigendecomposition. The principle components can be ranked by their corresponding eigenvalues, with the highest eigenvalues representing the components that capture the most variance in the data.\n",
    "\n",
    "We can then select the top k principle components that capture a certain percentage of the variance in the data. By selecting a smaller number of principle components, we can reduce the dimensionality of the dataset while preserving most of the important information. These principle components can then be used as the new set of features for further analysis, such as building a stock price prediction model.\n",
    "\n",
    "For example, let's say the original dataset contains 50 features, including company financial data, market trends, and other economic indicators. We can use PCA to extract the top 10 principle components that capture 90% of the variance in the data. These 10 principle components can then be used as the new set of features for building a stock price prediction model, reducing the dimensionality of the dataset and improving the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.**\n",
    "\n",
    "**Ans:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max = MinMaxScaler()\n",
    "min_max.fit_transform([[1, 5, 10, 15, 20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?**\n",
    "\n",
    "**Ans:**\n",
    "\n",
    "Performing Feature Extraction using PCA involves identifying the principle components that capture the most variance in the data and using them as the new set of features. In this case, the dataset contains five features: height, weight, age, gender, and blood pressure.\n",
    "\n",
    "Before applying PCA, we need to preprocess the data by standardizing it to have zero mean and unit variance. We can then compute the covariance matrix of the data and find the principle components using eigendecomposition.\n",
    "\n",
    "The number of principal components to retain depends on the percentage of variance we want to preserve in the data. A common rule of thumb is to choose the smallest number of principal components that capture at least 70-80% of the variance in the data.\n",
    "\n",
    "To determine the number of principal components to retain for this dataset, we can compute the explained variance ratio for each principle component, which represents the proportion of the total variance in the data that is explained by each component.\n",
    "\n",
    "Once we have computed the explained variance ratio for each component, we can plot a scree plot to visualize the proportion of variance explained by each principal component. The scree plot shows a diminishing returns relationship between the number of principal components and the amount of variance explained. We can then choose the number of principal components that capture a high proportion of the variance while avoiding overfitting the data.\n",
    "\n",
    "Without any knowledge of the dataset or its characteristics, it is difficult to determine the number of principal components that should be retained. However, as a general guideline, retaining 2-3 principal components may be a good starting point as they would capture the most significant variability in the data.\n",
    "\n",
    "Ultimately, the optimal number of principal components to retain depends on the specifics of the dataset and the analysis being performed. It may require some experimentation and evaluation to determine the optimal number of principal components to retain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
